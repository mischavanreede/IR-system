{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# RUN: 'pip install jsonlines' on: ModuleNotFoundError: No module named 'jsonlines'\n",
    "import json\n",
    "import jsonlines\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "# RUN: 'pip install whoosh' on: ModuleNotFoundError: No module named 'whoosh'\n",
    "import os, os.path\n",
    "\n",
    "#from whoosh import index\n",
    "import whoosh\n",
    "from whoosh.index import create_in\n",
    "from whoosh.fields import *\n",
    "from whoosh.writing import AsyncWriter\n",
    "\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "######################################\n",
    "\n",
    "class Indexer:\n",
    "    '''This class is used to create index of the WP database'''\n",
    "    \n",
    "    index_created = False\n",
    "    index_directory = \"\"\n",
    "    data_path = \"\"       \n",
    "    \n",
    "    def __init__(self):\n",
    "        print('Indexer has been initiated.')\n",
    "        \n",
    "    def indexCreated(self):\n",
    "        return self.index_created\n",
    "    \n",
    "    def openIndex(self):\n",
    "        pass\n",
    "      \n",
    "    ## Count documents\n",
    "    def countDocuments(self):\n",
    "        print(\"Counting documents, this might take a while...\")\n",
    "        counter = 0\n",
    "        tenKCounter = 0\n",
    "\n",
    "        with jsonlines.open(self.data_path) as reader:\n",
    "            for obj in tqdm(reader.iter(type=dict, skip_invalid=True)):\n",
    "                tenKCounter += 1\n",
    "                counter += 1\n",
    "            \n",
    "                if tenKCounter >= 10000:\n",
    "                    print(\"Current count is: \" + str(counter))\n",
    "                    tenKCounter = 0 \n",
    "        print(\"Last count is: \" + str(counter))\n",
    "        print(\"Counting done.\")\n",
    "    \n",
    "    def printDocumentData(self):\n",
    "        '''Prints every 10000th object'''\n",
    "        #dataframe = pd.DataFrame(columns = ['Title', 'Content', 'Year'])\n",
    "        with jsonlines.open(self.data_path) as reader:\n",
    "            counter = 0\n",
    "            for obj in tqdm(reader.iter(type=dict, skip_invalid=True)):\n",
    "                counter += 1\n",
    "                if (counter % 10000) == 0:\n",
    "                    print(\"Object number:\" + str(counter))\n",
    "                    print(type(obj))\n",
    "                    doc = Document(**obj)\n",
    "                    print(doc)\n",
    "        \n",
    "    def extractDocumentContents(self, contents):\n",
    "        '''Extracts document contents from array of dicts to string'''\n",
    "        build_string = \"\"\n",
    "        \n",
    "        for item in contents:\n",
    "            try:\n",
    "                if item['type'] == \"sanitized_html\":\n",
    "                    build_string = build_string + item['content']\n",
    "                    build_string = build_string + \"\\n\"\n",
    "            except:\n",
    "                build_string = \"Could not retreive content\"\n",
    "        \n",
    "        build_string = re.sub('<.*?>', '', build_string)\n",
    "        #print(build_string)\n",
    "        return build_string\n",
    "    \n",
    "    def extractDocumentDate(self, epochTimestamp):\n",
    "        '''Converst UNIX epoch timestamp to DATETIME'''\n",
    "        # TODO IMPLENT Default value on Handling Error: ValueError: invalid literal for int() with base 10: 'None'\n",
    "        if (epochTimestamp == ''):\n",
    "            return datetime(1,1,1)\n",
    "        try:\n",
    "            # Test if source is within specified dates\n",
    "            date_info = epochTimestamp\n",
    "            removed_zeros = str(date_info)[0:10]\n",
    "            timestamp = int(removed_zeros)\n",
    "            return datetime.fromtimestamp(timestamp, timezone('EST'))\n",
    "        except:\n",
    "            print(\"ERROR: In extractDocumentDate()\")\n",
    "            print(\"ERROR: \", sys.exc_info()[0])\n",
    "            #print(\"Caused by value: \" + str(date_info))\n",
    "            return datetime(1,1,1)\n",
    "    \n",
    "    def setIndexDirectory(self, directory):\n",
    "        self.index_directory = directory\n",
    "    \n",
    "    def getIndexLocation(self):\n",
    "        return self.index_directory\n",
    "    \n",
    "    def setDataPath(self, file_location):\n",
    "        self.data_path = file_location\n",
    "    \n",
    "    def getDataPath(self):\n",
    "        return self.data_path\n",
    "    \n",
    "    def setIndexCreatedTrue(self):\n",
    "        self.index_created = True        \n",
    "    \n",
    "    def index(self):\n",
    "        '''This method indexes the data'''\n",
    "        # https://whoosh.readthedocs.io/en/latest/api/writing.html\n",
    "        # https://appliedmachinelearning.blog/2018/07/31/developing-a-fast-indexing-and-full-text-search-engine-with-whoosh-a-pure-python-library/\n",
    "        # TODO add document contents to index\n",
    "        \n",
    "        #  [id, article_url, title, author, publised_date, contents, type, source]\n",
    "        # Define fields using whoosh's 'Schema' | https://whoosh.readthedocs.io/en/latest/schema.html#\n",
    "        # Can add field boost here\n",
    "        # TODO: critically look at different fields, which are important?, which need to be shown to the user? etc.\n",
    "        # TODO: check if index_location has been set\n",
    "        \n",
    "        assert self.index_directory and self.data_path # Both variables have to be set\n",
    "        \n",
    "        schema = Schema(doc_id = whoosh.fields.ID(unique=True, stored=True),\\\n",
    "                       article_url = whoosh.fields.STORED,\\\n",
    "                       title = whoosh.fields.TEXT(stored=True),\\\n",
    "                       author = whoosh.fields.ID,\\\n",
    "                       published_date = whoosh.fields.DATETIME,\\\n",
    "                       contents = whoosh.fields.TEXT)\n",
    "              \n",
    "        # Creating a index writer to add document as per schema\n",
    "        myindex = whoosh.index.create_in(self.index_directory,schema)\n",
    "        writer = whoosh.writing.AsyncWriter(myindex)\n",
    "        \n",
    "        # Build full index? Or partial\n",
    "        full_index = True\n",
    "        \n",
    "        counter = 0\n",
    "        fault_counter = 0\n",
    "        true_scores = []\n",
    "        \n",
    "        if full_index:\n",
    "            checker = 10000\n",
    "        else:\n",
    "            checker = 100\n",
    "            \n",
    "            print(\"Downloading query results.\")\n",
    "            print(\"This might take a few seconds...\")\n",
    "            \n",
    "            # read in true scores file\n",
    "            true_scores_url = \"https://trec.nist.gov/data/core/qrels2018.txt\"\n",
    "            true_scores_file = urllib.request.urlopen(true_scores_url)\n",
    "\n",
    "            for line in true_scores_file:\n",
    "                decoded_line = line.decode(\"utf-8\")\n",
    "                decoded_line = decoded_line.split(\" \")\n",
    "                decoded_line = decoded_line[2]\n",
    "                true_scores.append(decoded_line)\n",
    "                \n",
    "            print(\"Done downloading query results.\")\n",
    "        \n",
    "        # Loop over data\n",
    "        print(\"Looping over data. Indexing each article.\")\n",
    "        print(\"This might take a few minutes...\")\n",
    "        \n",
    "        with jsonlines.open(self.data_path) as reader:\n",
    "            for obj in tqdm(reader.iter(type=dict, skip_invalid=True)):\n",
    "                index_line = False\n",
    "                \n",
    "                if not full_index:\n",
    "                    for line in true_scores:\n",
    "                        if line == obj['id']:\n",
    "                            true_scores.remove(line)\n",
    "                            index_line = True\n",
    "                else:\n",
    "                    index_line = True\n",
    "                    \n",
    "                if index_line:\n",
    "                    retreived_date = self.extractDocumentDate(obj['published_date'])\n",
    "                    \n",
    "                    if retreived_date == datetime(1,1,1):\n",
    "                        fault_counter = fault_counter + 1\n",
    "                        continue\n",
    "                        \n",
    "                    writer.add_document(doc_id=obj['id'],\\\n",
    "                                        article_url=obj['article_url'],\\\n",
    "                                        title=obj['title'],\\\n",
    "                                        author=obj['author'],\\\n",
    "                                        published_date=retreived_date,\\\n",
    "                                        contents = self.extractDocumentContents(obj['contents']))\n",
    "                    \n",
    "                    counter = counter + 1\n",
    "                    if counter > checker:\n",
    "                        writer.commit()\n",
    "                        writer = whoosh.writing.AsyncWriter(myindex)\n",
    "                        \n",
    "                        if full_index:\n",
    "                            checker = checker + 10000\n",
    "                        else:\n",
    "                            checker = checker + 100\n",
    "                        \n",
    "                        clear_output(wait=True)\n",
    "                        \n",
    "                        print(\"Looping over data. Indexing each article.\")\n",
    "                        print(\"This might take a few minutes...\")\n",
    "                        print(\"Indexed \" + str(counter - 1) + \" articles\")\n",
    "                        \n",
    "                        if fault_counter > 0:\n",
    "                            print(\"Found \" + str(fault_counter) + \" wrongly formatted articles\")\n",
    "            \n",
    "            print(\"Looping complete.\")\n",
    "        print(\"Index created!\")\n",
    "        self.setIndexCreatedTrue()\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "# indexer = Indexer()\n",
    "# #indexer.countDocuments()\n",
    "# #indexer.printDocumentData()\n",
    "# #indexer.createDocumentDataStore()\n",
    "# indexer.index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "from whoosh.qparser import QueryParser\n",
    "from whoosh import scoring\n",
    "from whoosh.index import open_dir\n",
    "\n",
    "class Ranking:\n",
    "    '''This class contains functions that are used to create a ranking based on different algorithms'''\n",
    "    \n",
    "    show_n_results = 3\n",
    "    index_directory = \"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        print(\"Ranking class has been initiated.\")\n",
    "    \n",
    "    def indexCreated(self):\n",
    "        return indexCreated\n",
    "    \n",
    "    def createIndex(self):\n",
    "        pass\n",
    "    \n",
    "    def setIndexDirectory(self, directory):\n",
    "        self.index_directory = directory\n",
    "    \n",
    "    def openIndex(self):\n",
    "        assert self.index_directory\n",
    "        return open_dir(self.index_directory)\n",
    "    \n",
    "   \n",
    "    def resultsToList(self, results):\n",
    "        resultsList = []\n",
    "        for result in results:\n",
    "            resultsList.append(result.fields())\n",
    "        return resultsList\n",
    "            \n",
    "    \n",
    "    def searchTermFrequency(self, user_query, indexer):\n",
    "        '''Returns results for a given query based on the Term Frequency search algorithm. Returned value is a list of dictionaries.'''\n",
    "        #TODO check if index has been created\n",
    "        index_dir = indexer.getIndexLocation()\n",
    "        self.setIndexDirectory(index_dir)\n",
    "        index = self.openIndex()\n",
    "        resultsList = []        \n",
    "        # Add: whoosh.qparser.MultifieldParser(fieldnames, schema, fieldboosts=None, **kwargs)\n",
    "        # TODO: get fieldnames and schema from indexer -> add getters in Inderex class\n",
    "        # TODO: add parameter that dictates search algorithm\n",
    " \n",
    "        with index.searcher(weighting=scoring.Frequency) as searcher:\n",
    "            parsed_query = QueryParser(\"title\", index.schema).parse(user_query)\n",
    "            results = searcher.search(parsed_query, limit=self.show_n_results)\n",
    "            self.printResults(results)\n",
    "            \n",
    "    def searchTermFrequencyReturnResults(self, user_query):\n",
    "        #TODO check if index has been created\n",
    "        index = self.openIndex()\n",
    "        \n",
    "        with index.searcher(weighting=scoring.Frequency) as searcher:\n",
    "            parsed_query = QueryParser(\"title\", index.schema).parse(user_query)\n",
    "            results = searcher.search(parsed_query)            \n",
    "            return results\n",
    "            \n",
    "    \n",
    "    def searchTF_IDF():\n",
    "        '''Returns results for a given query based on the TF-IDF search algorithm. Returned value is a list of dictionaries.'''\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def searchBM25FReturnResults(self, user_query):\n",
    "        \n",
    "        index = self.openIndex()\n",
    "        \n",
    "        with index.searcher(weighting=scoring.BM25F(B=0.75, content_B=1.0, K1=1.5)) as searcher:\n",
    "            parsed_query = QueryParser(\"title\", index.schema).parse(user_query)\n",
    "            results = searcher.search(parsed_query)            \n",
    "            return results\n",
    "    def searchBM25F():\n",
    "        '''Returns results for a given query based on the BM25F search algorithm. Returned value is a list of dictionaries.'''\n",
    "        pass\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking class has been initiated.\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "import urllib\n",
    "\n",
    "class Evaluation:\n",
    "    '''Used to evaluate the ranking results'''\n",
    "    #should check if query used is in the \n",
    "    \n",
    "    # TODO load evaluation document and queries\n",
    "    # TODO pass queries on to ranking and obtain ranking results\n",
    "    # TODO compare ranking results to TREC evaluation\n",
    "    # TODO display results, how?\n",
    "    # TODO test different algorithms\n",
    "    \n",
    "    queries = []\n",
    "    ranking = Ranking()\n",
    "    \n",
    "    def load_queries(self):\n",
    "        \n",
    "        query_url = \"https://trec.nist.gov/data/core/topics2018.txt\"\n",
    "        query_file = urllib.request.urlopen(query_url)\n",
    "        titles, numbers = [], []\n",
    "        title = False\n",
    "        \n",
    "        for line in query_file:\n",
    "            decoded_line = line.decode(\"utf-8\")\n",
    "            \n",
    "            if title and not \"</title>\" in decoded_line:\n",
    "                titles.append(decoded_line.replace(\"\\n\", \"\").strip())\n",
    "\n",
    "            if \"<title>\" in decoded_line:\n",
    "                title = True\n",
    "            \n",
    "            if \"</title>\" in decoded_line:\n",
    "                title = False\n",
    "                \n",
    "            if \"<num>\" in decoded_line:\n",
    "                num = decoded_line.replace(\"<num>\", \"\").replace(\"</num>\", \"\").replace(\"\\n\", \"\").replace(\"Number: \", \"\").strip()\n",
    "                numbers.append(num)\n",
    "        \n",
    "        queries = list(zip(titles, numbers))\n",
    "        return queries \n",
    "\n",
    "        \n",
    "    def addResults(self, results_formatted, results, query, number):\n",
    "        count = 0\n",
    "        for hit in results:\n",
    "            current_result = str(number) + \" 0 \"                    \n",
    "            current_result = str(current_result) + str(hit.docnum) + \" \" + str(hit.score)           \n",
    "            results_formatted.append(current_result)\n",
    "            count += 1\n",
    "        print(\"for query {} there are {} hits\".format(query, count))\n",
    "        return results_formatted\n",
    "        \n",
    "        \n",
    "    def writeResults(self, results):\n",
    "        \n",
    "        with open(\"results.txt\", \"w\") as results_file:\n",
    "            for result in results:\n",
    "                results_file.write(result + \"\\n\")\n",
    "                \n",
    "    \n",
    "    def compareResults(self):        \n",
    "        # read in results.txt\n",
    "        predictions = []\n",
    "        with open(\"results.txt\", \"r\") as results_file:\n",
    "            for line in results_file:\n",
    "                current_result = line.split(\" \")\n",
    "                current_result = [i.strip(\"\\n\") for i in current_result]\n",
    "                \n",
    "                predictions.append(current_result)\n",
    "                \n",
    "        # read in true scores file\n",
    "        true_scores = []\n",
    "        true_scores_url = \"https://trec.nist.gov/data/core/qrels2018.txt\"\n",
    "        true_scores_file = urllib.request.urlopen(true_scores_url)\n",
    "\n",
    "        for line in true_scores_file:\n",
    "            decoded_line = line.decode(\"utf-8\")\n",
    "            decoded_line = decoded_line.split(\" \")\n",
    "            decoded_line = [i.strip(\"\\n\") for i in decoded_line]\n",
    "            \n",
    "            true_scores.append(decoded_line)\n",
    "        \n",
    "        \n",
    "        # compare results.txt (preds) with true scores\n",
    "        correct_preds, incorrect_preds = 0, 0\n",
    "        \n",
    "        for pred in predictions:\n",
    "            for true_score in true_scores:\n",
    "                \n",
    "                if pred[0] == true_score[0] and pred[2] == true_score[2]:\n",
    "                    if pred[3] == true_score[3]:\n",
    "                        correct_preds += 1\n",
    "                    else:\n",
    "                        incorrect_preds += 1\n",
    "                        \n",
    "                    break\n",
    "                    \n",
    "        if correct_preds == incorrect_preds == 0:\n",
    "            print(\"No matches found, because doc id's do not yet work\")\n",
    "\n",
    "        else:\n",
    "            print(\"Performance: {:.2f}%\".format(correct_preds/(correct_preds+incorrect_preds)))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "class UserInterface:\n",
    "    '''Class used to handle user interaction'''\n",
    "    # TODOS:\n",
    "    # implement method that asks used if \n",
    "    # implement methods that asks the user to set different options\n",
    "          \n",
    "    def __init__(self):\n",
    "        '''This is the constructor method of the UI'''\n",
    "        print(\"UserInterface has been initiated.\")       \n",
    "        \n",
    "    def getUserQuery(self):\n",
    "        ''''Retrieves the query from the user and returns it'''\n",
    "        userInput = input(\"Please enter a query: \")\n",
    "        return userInput\n",
    "    \n",
    "    def indexAlreadyCreated(self, directory_location):\n",
    "        print(\"Index is set.\")\n",
    "        print(\"Index files are stored in directory:\" + directory_location)\n",
    "        return\n",
    "    \n",
    "    def shouldCreateIndex(self):\n",
    "        print(\"Do you wish to create a new index?\")\n",
    "        userInput = input(\"Answer [y/n]: \")\n",
    "        if userInput == ('y' or 'Y' or \"yes\"):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def indexIsNotSet(self):\n",
    "        print(\"Index has not been set.\")\n",
    "        return\n",
    "    \n",
    "    def getIndexDirectory(self, default_directory):\n",
    "        userInput = input(\"Please enter a directory name for the index (default = /\"+ default_directory + \"): \")\n",
    "        if userInput == \"\":\n",
    "            return default_directory\n",
    "        return userInput\n",
    "    \n",
    "    def getDataPath(self, default_data_path):\n",
    "        print(\"Please enter the path to the TREC_Washington_Post_collection.v2.jl file.\")\n",
    "        print(\"The default location is: /\"+ default_data_path + \"): \")\n",
    "        userInput = input()\n",
    "        if userInput == \"\":\n",
    "            return default_data_path\n",
    "        return userInput\n",
    "    \n",
    "    def creatingIndex(self, index_directory, data_path):\n",
    "        pass\n",
    "    \n",
    "    def shouldAddExistingIndex(self):\n",
    "        print(\"Do you wish to add an existing index?\")\n",
    "        userInput = input(\"Answer [y/n]: \")\n",
    "        if userInput == ('y' or 'Y' or \"yes\"):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def stopSearchEngine(self):\n",
    "        print(\"Search Engine is stopped.\")\n",
    "        \n",
    "    def shouldTerminateSearchEngine(self):\n",
    "        print(\"Do you wish to stop the search engine?\")\n",
    "        userInput = input(\"Answer [y/n]: \")\n",
    "        if userInput == ('y' or 'Y' or \"yes\"):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def printResults(self, results):\n",
    "        print(\"\\nPrinting results:\")\n",
    "        for result in results:\n",
    "            print(result)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block used to run everything:\n",
    "# TODO expand this 'Controll' class that handles information flow\n",
    "# TODO add querying functionality\n",
    "# TODO implement choice of search algorithm\n",
    "# TODO implement evaluation mode\n",
    "import time\n",
    "\n",
    "class SearchEngine:\n",
    "    '''This class embodies the search engine and acts a a controller class'''\n",
    "    \n",
    "    UI = None\n",
    "    Indexer = None\n",
    "    Ranking = None\n",
    "    \n",
    "    RUNNING = False\n",
    "    STOPPED = False\n",
    "    EVALUATION_MODE = True\n",
    "    USER_MODE = False\n",
    "    \n",
    "    user_query = \"\"\n",
    "    DEFAULT_data_path = 'WP-corpus/data/TREC_Washington_Post_collection.v2.jl'\n",
    "    DEFAULT_index_directory = \"indexdir\"\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        print(\"Search Engine has been initiated.\")\n",
    "    \n",
    "    def stopSearchEngine(self, UI):\n",
    "        self.RUNNING = False\n",
    "        UI.stopSearchEngine()        \n",
    "        \n",
    "    def setIndex(self, Indexer, UI):\n",
    "        if Indexer.indexCreated():\n",
    "            directory_location = Indexer.getIndexLocation()\n",
    "            UI.indexAlreadyCreated(directory_location)\n",
    "            return\n",
    "        else:\n",
    "            UI.indexIsNotSet()\n",
    "            if UI.shouldCreateIndex(): # This is for creating a new index\n",
    "                # Index directory (setDir)\n",
    "                index_directory = UI.getIndexDirectory(self.DEFAULT_index_directory)\n",
    "                Indexer.setIndexDirectory(index_directory)\n",
    "                # Check if index_dir exist and makes one if it doesn't\n",
    "                if not os.path.exists(index_directory):\n",
    "                    os.mkdir(index_directory)\n",
    "                \n",
    "                # Data file path (setPath)    \n",
    "                data_path = UI.getDataPath(self.DEFAULT_data_path)\n",
    "                Indexer.setDataPath(data_path)\n",
    "                \n",
    "                # Creating Index\n",
    "                UI.creatingIndex(index_directory, data_path)\n",
    "                Indexer.index()\n",
    "                return\n",
    "                \n",
    "            elif UI.shouldAddExistingIndex(): # This is for adding an existing index\n",
    "                \n",
    "                # TODO communicate chosen directories and chosen data file\n",
    "                # TODO add checks to check for existence of index?\n",
    "                \n",
    "                # Index directory, duplicate code -> (setDir)\n",
    "                index_directory = UI.getIndexDirectory(self.DEFAULT_index_directory)\n",
    "                Indexer.setIndexDirectory(index_directory)\n",
    "                # Check if index_dir exist and makes one if it doesn't\n",
    "                if not os.path.exists(index_directory):\n",
    "                    os.mkdir(index_directory)\n",
    "                \n",
    "                # Data file path, duplicate code -> (setPath)    \n",
    "                data_path = UI.getDataPath(self.DEFAULT_data_path)\n",
    "                Indexer.setDataPath(data_path)\n",
    "                \n",
    "                # Set index created to true\n",
    "                Indexer.setIndexCreatedTrue()\n",
    "                return\n",
    "                                \n",
    "            else: # TODO: Ask if the user wants to continue or stop\n",
    "                if UI.shouldTerminateSearchEngine(): \n",
    "                    self.STOPPED = True\n",
    "                return\n",
    "            return\n",
    "       \n",
    "    def run(self):\n",
    "        '''This function start the search engine'''\n",
    "        print('Search Engine started.')\n",
    "        \n",
    "        self.UI = UserInterface()\n",
    "        self.Indexer = Indexer()\n",
    "        self.Ranking = Ranking()\n",
    "                \n",
    "        self.RUNNING = True\n",
    "        while self.RUNNING:\n",
    "            if not self.Indexer.indexCreated():\n",
    "                #TODO Inform user that an index needs to be set\n",
    "                self.setIndex(self.Indexer, self.UI)\n",
    "                if self.STOPPED == True: # Stop SE if user did not want to continue\n",
    "                    self.stopSearchEngine(self.UI)\n",
    "                    break\n",
    "            #TODO ask user to choose a mode\n",
    "                       \n",
    "            if self.EVALUATION_MODE:\n",
    "            # This is used to evaluate SE against the TREC relevance judgements\n",
    "                index_dir = self.Indexer.getIndexLocation()\n",
    "                self.Ranking.setIndexDirectory(index_dir)\n",
    "                \n",
    "                evaluation = Evaluation()\n",
    "                queries  = evaluation.load_queries()\n",
    "                print(\"Queries loaded\")\n",
    "                \n",
    "                results_formatted = []\n",
    "                \n",
    "                for (query, number) in tqdm(queries):\n",
    "                    #results = self.Ranking.searchTermFrequencyReturnResults(query)\n",
    "                    results = self.Ranking.searchBM25FReturnResults(query)\n",
    "                    results_formatted = evaluation.addResults(results_formatted, results, query, number)\n",
    "                \n",
    "                evaluation.writeResults(results_formatted)\n",
    "                evaluation.compareResults()    \n",
    "                \n",
    "                print(\"Done\")\n",
    "                self.RUNNING = False\n",
    " \n",
    "            if self.USER_MODE:\n",
    "                # This is used to query questions\n",
    "                #TODO ask for search algorithm\n",
    "                #TODO ask for user query\n",
    "                #TODO move printing results to UI\n",
    "                print(\"Entering USER_MODE\")\n",
    "                user_query = self.UI.getUserQuery()\n",
    "                print(\"Entered query: \" + user_query)\n",
    "                \n",
    "\n",
    "                results = self.Ranking.searchTermFrequency(user_query, self.Indexer)\n",
    "                self.UI.printResults(results)\n",
    "                \n",
    "            print(\"Starting over.\")\n",
    "            time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "590070it [11:19, 761.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looping over data. Indexing each article.\n",
      "This might take a few minutes...\n",
      "Indexed 590000 articles\n",
      "Found 9 wrongly formatted articles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "595025it [11:26, 866.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looping complete.\n",
      "Index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queries loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|█▋                                                                                 | 1/50 [00:03<02:43,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query Women in Parliaments there are 0 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|███▎                                                                               | 2/50 [00:03<01:57,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query Black Bear Attacks there are 0 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|████▉                                                                              | 3/50 [00:04<01:25,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query Airport Security there are 10 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|██████▋                                                                            | 4/50 [00:04<01:03,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query Wildlife Extinction there are 0 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████████▎                                                                          | 5/50 [00:04<00:47,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query Health and Computer Terminals there are 0 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█████████▉                                                                         | 6/50 [00:05<00:37,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query human smuggling there are 1 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|███████████▌                                                                       | 7/50 [00:05<00:29,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query transportation tunnel disasters there are 0 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|█████████████▎                                                                     | 8/50 [00:05<00:24,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query piracy there are 10 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|██████████████▉                                                                    | 9/50 [00:06<00:21,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query hydrogen energy there are 1 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████████████▍                                                                 | 10/50 [00:06<00:18,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query euro opposition there are 0 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|██████████████████                                                                | 11/50 [00:06<00:18,  2.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query mercy killing there are 0 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|███████████████████▋                                                              | 12/50 [00:07<00:15,  2.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query automobile recalls there are 0 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|█████████████████████▎                                                            | 13/50 [00:07<00:15,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query Amazon rain forest there are 1 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|██████████████████████▉                                                           | 14/50 [00:07<00:13,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query tropical storms there are 4 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|████████████████████████▌                                                         | 15/50 [00:08<00:14,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query Cuba, sugar, exports there are 0 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|██████████████████████████▏                                                       | 16/50 [00:08<00:13,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query art, stolen, forged there are 0 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 34%|███████████████████████████▉                                                      | 17/50 [00:09<00:12,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query law enforcement, dogs there are 0 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|█████████████████████████████▌                                                    | 18/50 [00:09<00:11,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query UV damage, eyes there are 0 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███████████████████████████████▏                                                  | 19/50 [00:09<00:10,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query Greek, philosophy, stoicism there are 0 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████████████████████████████████▊                                                 | 20/50 [00:10<00:10,  2.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query inventions, scientific discoveries there are 0 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|██████████████████████████████████▍                                               | 21/50 [00:10<00:09,  3.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query heroic acts there are 0 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|████████████████████████████████████                                              | 22/50 [00:10<00:09,  3.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query women clergy there are 3 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|█████████████████████████████████████▋                                            | 23/50 [00:11<00:08,  3.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query human stampede there are 0 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|███████████████████████████████████████▎                                          | 24/50 [00:11<00:07,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query food stamps increase there are 0 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████████████████                                         | 25/50 [00:11<00:07,  3.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query college education advantage there are 0 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|██████████████████████████████████████████▋                                       | 26/50 [00:11<00:07,  3.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query Africa polio vaccination there are 0 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|████████████████████████████████████████████▎                                     | 27/50 [00:12<00:07,  2.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query women driving in Saudi Arabia there are 0 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|█████████████████████████████████████████████▉                                    | 28/50 [00:12<00:08,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query declining middle class in U.S. there are 0 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|███████████████████████████████████████████████▌                                  | 29/50 [00:13<00:07,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query \"Women on 20s\" there are 0 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|█████████████████████████████████████████████████▏                                | 30/50 [00:13<00:07,  2.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query eating invasive species there are 1 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|██████████████████████████████████████████████████▊                               | 31/50 [00:13<00:07,  2.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query computers and paralyzed people there are 0 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|████████████████████████████████████████████████████▍                             | 32/50 [00:14<00:07,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query Chavez medical treatment in Cuba there are 0 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 66%|██████████████████████████████████████████████████████                            | 33/50 [00:14<00:07,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query Boston marathon bombing verdict there are 0 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|███████████████████████████████████████████████████████▊                          | 34/50 [00:15<00:06,  2.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query protect Earth from asteroids there are 0 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|█████████████████████████████████████████████████████████▍                        | 35/50 [00:15<00:06,  2.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query diabetes and toxic chemicals there are 0 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|███████████████████████████████████████████████████████████                       | 36/50 [00:16<00:05,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query car hacking there are 1 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|████████████████████████████████████████████████████████████▋                     | 37/50 [00:16<00:05,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query social media and teen suicide there are 0 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|██████████████████████████████████████████████████████████████▎                   | 38/50 [00:17<00:05,  2.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query marijuana potency there are 1 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|███████████████████████████████████████████████████████████████▉                  | 39/50 [00:17<00:04,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query China one-child impact there are 0 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|█████████████████████████████████████████████████████████████████▌                | 40/50 [00:17<00:04,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query Jason Rezaian released from Iran there are 0 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|███████████████████████████████████████████████████████████████████▏              | 41/50 [00:18<00:04,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query federal minimum wage increase there are 0 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|████████████████████████████████████████████████████████████████████▉             | 42/50 [00:18<00:03,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query Alan Gross released by Cuba there are 0 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|██████████████████████████████████████████████████████████████████████▌           | 43/50 [00:19<00:02,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query eggs in a healthy diet there are 0 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|████████████████████████████████████████████████████████████████████████▏         | 44/50 [00:19<00:02,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query U.S. age demographics there are 0 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████████████████████████████████████████████████████████████████████▊        | 45/50 [00:19<00:01,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query bacterial infection mortality rate there are 0 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|███████████████████████████████████████████████████████████████████████████▍      | 46/50 [00:20<00:01,  2.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query email scams there are 0 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|█████████████████████████████████████████████████████████████████████████████     | 47/50 [00:20<00:01,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query Sony cyberattack there are 5 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|██████████████████████████████████████████████████████████████████████████████▋   | 48/50 [00:20<00:00,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query control of MRSA there are 0 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 98%|████████████████████████████████████████████████████████████████████████████████▎ | 49/50 [00:21<00:00,  2.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query Bezos purchases Washington Post there are 0 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [00:21<00:00,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query ethanol and food prices there are 0 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No matches found, because doc id's do not yet work\n",
      "Done\n",
      "Starting over.\n"
     ]
    }
   ],
   "source": [
    "mySearchEngine = SearchEngine()\n",
    "mySearchEngine.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
