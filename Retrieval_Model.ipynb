{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# RUN: 'pip install jsonlines' on: ModuleNotFoundError: No module named 'jsonlines'\n",
    "import json\n",
    "import jsonlines\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# RUN: 'pip install whoosh' on: ModuleNotFoundError: No module named 'whoosh'\n",
    "import os, os.path\n",
    "\n",
    "#from whoosh import index\n",
    "import whoosh\n",
    "from whoosh.index import create_in\n",
    "from whoosh.fields import *\n",
    "from whoosh.writing import AsyncWriter\n",
    "\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "######################################\n",
    "\n",
    "class Indexer:\n",
    "    '''This class is used to create index of the WP database'''\n",
    "    \n",
    "    index_created = False\n",
    "    index_directory = \"\"\n",
    "    data_path = \"\"\n",
    "    # [id, article_url, title, author, publised_date, contents, type, source]\n",
    "    # Define fields using whoosh's 'Schema' | https://whoosh.readthedocs.io/en/latest/schema.html#\n",
    "    # Can add field boost here\n",
    "    schema = Schema(doc_id = whoosh.fields.ID(unique=True, stored=True),\\\n",
    "                       article_url = whoosh.fields.STORED,\\\n",
    "                       title = whoosh.fields.TEXT(stored=True),\\\n",
    "                       author = whoosh.fields.ID,\\\n",
    "                       published_date = whoosh.fields.DATETIME,\\\n",
    "                       contents = whoosh.fields.TEXT)\n",
    "    \n",
    "    def __init__(self):\n",
    "        print('Indexer has been initiated.')\n",
    "        \n",
    "    def indexCreated(self):\n",
    "        return self.index_created\n",
    "      \n",
    "    ## Count documents\n",
    "    def countDocuments(self):\n",
    "        print(\"Counting documents, this might take a while...\")\n",
    "        counter = 0\n",
    "        tenKCounter = 0\n",
    "\n",
    "        with jsonlines.open(self.data_path) as reader:\n",
    "            for obj in tqdm(reader.iter(type=dict, skip_invalid=True)):\n",
    "                tenKCounter += 1\n",
    "                counter += 1\n",
    "            \n",
    "                if tenKCounter >= 10000:\n",
    "                    print(\"Current count is: \" + str(counter))\n",
    "                    tenKCounter = 0 \n",
    "        print(\"Last count is: \" + str(counter))\n",
    "        print(\"Counting done.\")\n",
    "        \n",
    "    def extractDocumentContents(self, contents):\n",
    "        '''Extracts document contents from array of dicts to string'''\n",
    "        build_string = \"\"\n",
    "        json_array = json.dumps(str(contents))\n",
    "        \n",
    "        for item in json_array:\n",
    "            try:\n",
    "                print(str(item['type']))\n",
    "                if str(item['type']) == \"sanitized_html\":\n",
    "                    build_string = build_string + str(item['content'])\n",
    "                    build_string = build_string + \"\\n\"\n",
    "            except:\n",
    "                build_string = \"Could not retreive content\"\n",
    "        \n",
    "        print(build_string)\n",
    "        return build_string\n",
    "    \n",
    "    def extractDocumentDate(self, epochTimestamp):\n",
    "        '''Converst UNIX epoch timestamp to DATETIME'''\n",
    "        if (epochTimestamp == ''):\n",
    "            return datetime(1,1,1)\n",
    "        try:\n",
    "            # Test if source is within specified dates\n",
    "            date_info = epochTimestamp\n",
    "            removed_zeros = str(date_info)[0:10]\n",
    "            timestamp = int(removed_zeros)\n",
    "            return datetime.fromtimestamp(timestamp, timezone('EST'))\n",
    "        except:\n",
    "            print(\"ERROR: In extractDocumentDate()\")\n",
    "            print(\"ERROR: \", sys.exc_info()[0])\n",
    "            #print(\"Caused by value: \" + str(date_info))\n",
    "            return datetime(1,1,1)\n",
    "    \n",
    "    def setIndexDirectory(self, directory):\n",
    "        self.index_directory = directory\n",
    "    \n",
    "    def getIndexLocation(self):\n",
    "        return self.index_directory\n",
    "    \n",
    "    def setDataPath(self, file_location):\n",
    "        self.data_path = file_location\n",
    "    \n",
    "    def getDataPath(self):\n",
    "        return self.data_path\n",
    "    \n",
    "    def setIndexCreatedTrue(self):\n",
    "        self.index_created = True\n",
    "    \n",
    "    def getSchema(self):\n",
    "        return self.schema\n",
    "    \n",
    "    def index(self):\n",
    "        '''This method indexes the data'''\n",
    "        # https://whoosh.readthedocs.io/en/latest/api/writing.html\n",
    "        # https://appliedmachinelearning.blog/2018/07/31/developing-a-fast-indexing-and-full-text-search-engine-with-whoosh-a-pure-python-library/\n",
    "        \n",
    "        assert self.index_directory and self.data_path # Both variables have to be set\n",
    "        \n",
    "        schema = self.getSchema()\n",
    "              \n",
    "        # Creating a index writer to add document as per schema\n",
    "        myindex = whoosh.index.create_in(self.index_directory, schema)\n",
    "        writer = whoosh.writing.AsyncWriter(myindex)\n",
    "        \n",
    "        # Loop over data\n",
    "        print(\"Looping over data. Indexing each article.\")\n",
    "        print(\"This might take a few minutes...\")\n",
    "        counter = 0\n",
    "        checker = 10000\n",
    "        fault_counter = 0\n",
    "        with jsonlines.open(self.data_path) as reader:\n",
    "            for obj in tqdm(reader.iter(type=dict, skip_invalid=True)):\n",
    "                retreived_date = self.extractDocumentDate(obj['published_date'])\n",
    "                if retreived_date != datetime(1,1,1):\n",
    "                    counter = counter + 1\n",
    "                    writer.add_document(doc_id=obj['id'],\\\n",
    "                                        article_url=obj['article_url'],\\\n",
    "                                        title=obj['title'],\\\n",
    "                                        author=obj['author'],\\\n",
    "                                        published_date=retreived_date,\\\n",
    "                                        contents = self.extractDocumentContents(obj['contents']))\n",
    "                else:\n",
    "                    fault_counter = fault_counter + 1\n",
    "                    \n",
    "                if counter > checker:\n",
    "                    writer.commit()\n",
    "                    writer = whoosh.writing.AsyncWriter(myindex)\n",
    "                    checker = checker + 10000\n",
    "                    clear_output(wait=True)\n",
    "                    print(\"Looping over data. Indexing each article.\")\n",
    "                    print(\"This might take a few minutes...\")\n",
    "                    print(\"Indexed \" + str(counter - 1) + \" articles\")\n",
    "                    break\n",
    "                    if fault_counter > 0:\n",
    "                        print(\"Found \" + str(fault_counter) + \" wrongly formatted articles\")\n",
    "            \n",
    "            print(\"Looping complete.\")\n",
    "        print(\"Index created!\")\n",
    "        self.setIndexCreatedTrue()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "from whoosh.qparser import QueryParser, MultifieldParser\n",
    "from whoosh import scoring\n",
    "from whoosh.index import open_dir\n",
    "\n",
    "class Ranking:\n",
    "    '''This class contains functions that are used to create a ranking based on different algorithms'''\n",
    "    \n",
    "    show_n_results = 350\n",
    "    index_directory = \"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        print(\"Ranking class has been initiated.\")\n",
    "    \n",
    "    def indexCreated(self):\n",
    "        return indexCreated\n",
    "    \n",
    "    def setIndexDirectory(self, directory):\n",
    "        self.index_directory = directory\n",
    "    \n",
    "    def openIndex(self):\n",
    "        assert self.index_directory\n",
    "        return open_dir(self.index_directory)\n",
    "    \n",
    "   \n",
    "    def resultsToList(self, results):\n",
    "        results_list = []\n",
    "        for result in results:\n",
    "            result_dict = result.fields()\n",
    "            result_dict['score'] = result.score\n",
    "            results_list.append(result_dict)\n",
    "        return results_list\n",
    "            \n",
    "    \n",
    "    def searchWithSelectedAlgorithm(self, user_query, indexer, scoring_algorithm):\n",
    "    #TODO check if index has been created\n",
    "        index_dir = indexer.getIndexLocation()\n",
    "        self.setIndexDirectory(index_dir)\n",
    "        index = self.openIndex()\n",
    "        results_list = []\n",
    "        schema = indexer.getSchema()\n",
    "        fields = schema.scorable_names()\n",
    "        \n",
    "        with index.searcher(weighting=scoring_algorithm) as searcher:\n",
    "            #parsed_query = QueryParser(\"title\", index.schema).parse(user_query)\n",
    "            parsed_query = MultifieldParser(fields, schema).parse(user_query)\n",
    "            results = searcher.search(parsed_query, limit=self.show_n_results)\n",
    "            results_list = self.resultsToList(results)\n",
    "        \n",
    "        return results_list\n",
    "    \n",
    "    def searchTermFrequency(self, user_query, indexer):\n",
    "        '''Returns results for a given query based on the TF-IDF search algorithm. Returned value is a list of dictionaries.'''\n",
    "        scoring_algorithm = scoring.Frequency\n",
    "        return self.searchWithSelectedAlgorithm(user_query, indexer, scoring_algorithm)            \n",
    "    \n",
    "    def searchTF_IDF(self, user_query, indexer):\n",
    "        '''Returns results for a given query based on the TF-IDF search algorithm. Returned value is a list of dictionaries.'''\n",
    "        scoring_algorithm = scoring.TF_IDF\n",
    "        return self.searchWithSelectedAlgorithm(user_query, indexer, scoring_algorithm)\n",
    "    \n",
    "    def searchBM25F(self, user_query, indexer):\n",
    "        '''Returns results for a given query based on the BM25F search algorithm. Returned value is a list of dictionaries.'''\n",
    "        scoring_algorithm = scoring.BM25F(B=0.75, content_B=1.0, K1=1.5)\n",
    "        return self.searchWithSelectedAlgorithm(user_query, indexer, scoring_algorithm)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "################################################\n",
    "import urllib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class Evaluation:\n",
    "    '''Used to evaluate the ranking results'''\n",
    "    #should check if query used is in the \n",
    "    \n",
    "    # TODO load evaluation document and queries\n",
    "    # TODO pass queries on to ranking and obtain ranking results\n",
    "    # TODO compare ranking results to TREC evaluation\n",
    "    # TODO display results, how?\n",
    "    # TODO test different algorithms\n",
    "    \n",
    "    queries = []\n",
    "    query_url = \"https://trec.nist.gov/data/core/topics2018.txt\"\n",
    "    \n",
    "    def load_queries(self):\n",
    "        '''Returns (query, number) pair where number is the query ID'''\n",
    "        \n",
    "        query_file = urllib.request.urlopen(self.query_url)\n",
    "        titles, numbers = [], []\n",
    "        title = False\n",
    "        \n",
    "        for line in query_file:\n",
    "            decoded_line = line.decode(\"utf-8\")\n",
    "            \n",
    "            if title and not \"</title>\" in decoded_line:\n",
    "                titles.append(decoded_line.replace(\"\\n\", \"\").strip())\n",
    "\n",
    "            if \"<title>\" in decoded_line:\n",
    "                title = True\n",
    "            \n",
    "            if \"</title>\" in decoded_line:\n",
    "                title = False\n",
    "                \n",
    "            if \"<num>\" in decoded_line:\n",
    "                num = decoded_line.replace(\"<num>\", \"\").replace(\"</num>\", \"\").replace(\"\\n\", \"\").replace(\"Number: \", \"\").strip()\n",
    "                numbers.append(num)\n",
    "        \n",
    "        queries = list(zip(titles, numbers))\n",
    "        return queries \n",
    "\n",
    "        \n",
    "    def addResults(self, results, results_formatted, query, query_id):\n",
    "        count = 0\n",
    "        for hit in results: \n",
    "            score = int(hit['score'])\n",
    "                \n",
    "            current_result = str(query_id) + \" 0 \"  + str(hit[\"doc_id\"]) + \" \" + str(score)           \n",
    "            results_formatted.append(current_result)\n",
    "            \n",
    "            count += 1\n",
    "        #print(\"for query {} there are {} hits\".format(query, count))\n",
    "        return results_formatted\n",
    "        \n",
    "        \n",
    "    def writeResults(self, results):\n",
    "        \n",
    "        with open(\"results.txt\", \"w\") as results_file:\n",
    "            for result in results:\n",
    "                results_file.write(result + \"\\n\")\n",
    "    \n",
    "    def plotResults(self):\n",
    "        \n",
    "        all_scores = []\n",
    "        \n",
    "        with open(\"results.txt\", \"r\") as results_file:\n",
    "            for line in results_file:\n",
    "                current_result = line.split(\" \")\n",
    "                current_result = [i.strip(\"\\n\") for i in current_result]                    \n",
    "                all_scores.append(int(float(current_result[-1])))\n",
    "                \n",
    "        _ = plt.hist(all_scores, bins='auto')  \n",
    "        plt.title(\"Histogram with 'auto' bins\")\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        print(\"Max score \", max(all_scores))\n",
    "        print(\"Min score \", min(all_scores))\n",
    "        \n",
    "    def getRelevantDocumentsFromTREC(self):\n",
    "        '''Returns a dict of {ID, [documents]} where the ID indicates the query ID and documents contains a list of relevant documents.'''\n",
    "        #NOTE: this does NOT take into account differences between 1 and 2 judgments\n",
    "        #Can add tuple of (doc_id, score) in dict: {Query_id: [(doc_id, score)]}\n",
    "        # read in true scores file\n",
    "        true_scores = []\n",
    "        true_scores_url = \"https://trec.nist.gov/data/core/qrels2018.txt\"\n",
    "        true_scores_file = urllib.request.urlopen(true_scores_url)\n",
    "        stored_lines = []\n",
    "        for line in true_scores_file:\n",
    "            decoded_line = line.decode(\"utf-8\")\n",
    "            decoded_line = decoded_line.split(\" \")\n",
    "            decoded_line = [i.strip(\"\\n\") for i in decoded_line]\n",
    "            \n",
    "            stored_lines.append([decoded_line[0], decoded_line[2], decoded_line[3]]) # Creates list of lines of [query_id, document_id, judgement]\n",
    "        \n",
    "        #Init dict\n",
    "        relevant_documents_dict = {}\n",
    "        for line in stored_lines:\n",
    "            query_id = line[0]\n",
    "            relevant_documents_dict[query_id] = []   \n",
    "        #Fill dict\n",
    "        for line in stored_lines:\n",
    "            query_id = line[0]\n",
    "            document_id = line[1]\n",
    "            judgement = line[2]\n",
    "            if judgement == '1' or judgement == '2':\n",
    "                relevant_documents_dict[query_id].append(document_id)    \n",
    "        #print(relevant_documents_dict)\n",
    "        return relevant_documents_dict\n",
    "    \n",
    "    def getResultsFromTXT(self):\n",
    "        # read in results.txt\n",
    "        #Can add tuple of (doc_id, score) in dict: {Query_id: [(doc_id, score)]}, but should convert score\n",
    "        stored_lines = []\n",
    "        with open(\"results.txt\", \"r\") as results_file:\n",
    "            for line in results_file:\n",
    "                current_result = line.split(\" \")\n",
    "                current_result = [i.strip(\"\\n\") for i in current_result]\n",
    "                \n",
    "                stored_lines.append([current_result[0], current_result[2], current_result[3]]) # Creates list of lines of [query_id, document_id, judgement]\n",
    "        # Init dict\n",
    "        result_documents_dict = {}\n",
    "        for line in stored_lines:\n",
    "            query_id = line[0]\n",
    "            result_documents_dict[query_id] = []   \n",
    "        #Fill dict\n",
    "        for line in stored_lines:\n",
    "            query_id = line[0]\n",
    "            document_id = line[1]\n",
    "            judgement = line[2]\n",
    "            result_documents_dict[query_id].append(document_id)\n",
    "        #print(result_documents_dict)\n",
    "        return result_documents_dict\n",
    "    \n",
    "    def calculateFmeasure(self, recall, precision, B=1):\n",
    "        if (recall + precision) == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return ((B*B + 1)*precision*recall)/((B*B)*precision + recall)\n",
    "    \n",
    "    def calculateEvaluationMeasuresPerQuery(self, search_engine_results_dict, relevant_documents_dict):\n",
    "        resultEvalList = [] #holds results for each query\n",
    "        for query_id, documents in relevant_documents_dict.items():       \n",
    "            true_positives = 0\n",
    "            false_positives = 0\n",
    "            false_negatives = 0\n",
    "            #calculates tp, fp, fn\n",
    "            if query_id in search_engine_results_dict.keys(): #SE results contains matches for this query\n",
    "                for document in search_engine_results_dict[query_id]:\n",
    "                    if document in relevant_documents_dict[query_id]: \n",
    "                        true_positives += 1\n",
    "                    else: # document from results is not relevant\n",
    "                        false_positives += 1\n",
    "                # false negatives are number of documents that are relevant but are not in the results\n",
    "                # so, number of relevant documents minus the found matches\n",
    "                false_negatives = len(set(relevant_documents_dict[query_id]) - set(search_engine_results_dict[query_id]))\n",
    "            \n",
    "            else: # SE results did not contain matches for current query\n",
    "                true_positives = 0\n",
    "                false_positives = 0\n",
    "                false_negatives = len(relevant_documents_dict[query_id])\n",
    "            \n",
    "            # Recall: (number of retrieved docs that are relevant) / (number of retrieved docs)\n",
    "            if (true_positives + false_positives) != 0: # avoid dividing by 0\n",
    "                recall = true_positives / (true_positives + false_positives) \n",
    "            else:\n",
    "                recall = 0\n",
    "            \n",
    "            # Precision: (number of retrieved docs that are relevant) / (number of relevant docs)\n",
    "            if (true_positives + false_negatives) != 0: # avoid dividing by 0\n",
    "                precision = true_positives / (true_positives + false_negatives)\n",
    "            else:\n",
    "                precision = 0\n",
    "            \n",
    "            f_1_measure = self.calculateFmeasure(recall, precision, 1)\n",
    "            \n",
    "            queryEval = {}\n",
    "            queryEval['query_id'] = query_id\n",
    "            queryEval['true_positives'] = true_positives\n",
    "            queryEval['false_positives'] = false_positives\n",
    "            queryEval['false_negatives'] = false_negatives\n",
    "            queryEval['recall'] = recall # round(recall, 4) #round to 4 decimals\n",
    "            queryEval['precision'] = precision #round(precision, 4)\n",
    "            queryEval['f_1_measure'] = f_1_measure #round(f_1_measure, 4)\n",
    "            \n",
    "            resultEvalList.append(queryEval)\n",
    "        return resultEvalList\n",
    "    \n",
    "    def evaluateResults(self):\n",
    "                \n",
    "        search_engine_results_dict = self.getResultsFromTXT() # contains results from our search engine\n",
    "        relevant_documents_dict = self.getRelevantDocumentsFromTREC() # contains relevant results defined by TREC\n",
    "        \n",
    "        resultList = self.calculateEvaluationMeasuresPerQuery(search_engine_results_dict, relevant_documents_dict)\n",
    "        \n",
    "        recall_list = []\n",
    "        precision_list = []\n",
    "        f_measure_list = []\n",
    "        for result in resultList:\n",
    "            recall_list.append(result['recall'])\n",
    "            precision_list.append(result['precision'])\n",
    "            f_measure_list.append(result['f_1_measure'])\n",
    "        \n",
    "        average_recall= sum(recall_list) / len(recall_list)\n",
    "        average_precision= sum(precision_list) / len(precision_list)\n",
    "        average_f_measure= sum(f_measure_list) / len(f_measure_list)\n",
    "        \n",
    "        # calculate average precision\n",
    "\n",
    "        print(\"Average Recall\", average_recall)\n",
    "        print(\"Average Precision\", average_precision)\n",
    "        print(\"Average F_1-Measure\", average_f_measure)\n",
    "\n",
    "# # ---------------\n",
    "# #             all_true_scores.append(int(decoded_line[-1]))\n",
    "            \n",
    "# #             true_scores.append(decoded_line)\n",
    "# #             scores_per_query.append(int(decoded_line[-1]))\n",
    "# #             if judgement == '1' or judgement == '2':\n",
    "# #                 judgements_per_query.append(query_id)\n",
    "    \n",
    "# #             if query != current_query:\n",
    "# #                 if count != 0:\n",
    "# #                     judgements_per_query.append(count)\n",
    "# #                     count = 0\n",
    "# #                 current_query = query\n",
    "                \n",
    "                \n",
    "# #             else:\n",
    "# #                 judgements_per_query.append(decoded_line[0])\n",
    "# #                 count += 1\n",
    "            \n",
    "#         _ = plt.hist(judgements_per_query, bins=50)\n",
    "#         plt.title(\"Nr of judgements per query\")\n",
    "#         plt.xticks(rotation='vertical')\n",
    "#         plt.show()\n",
    "        \n",
    "# #         _ = plt.hist(all_true_scores, bins='auto')  \n",
    "# #         plt.title(\"Histogram true relevance scores with 'auto' bins\")\n",
    "        \n",
    "# #         plt.show()\n",
    "# #         print(\"In total there are {} query-doc pairs\".format(len(all_true_scores)))\n",
    "# #         print(\"of which {}: 0, {}: 1, {}: 2\".format(all_true_scores.count(0), all_true_scores.count(1), all_true_scores.count(2)))\n",
    "#         # compare results.txt (preds) with true scores\n",
    "#         #correct_preds, incorrect_preds = 0, 0\n",
    "        \n",
    "#         best_performance = 0\n",
    "        \n",
    "        \n",
    "#         for lower_bound in [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24]:\n",
    "#             for upper_bound in [26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48]:\n",
    "                \n",
    "#                 correct_preds, incorrect_preds = 0, 0\n",
    "#                 matches_found = 0\n",
    "#                 for pred in predictions:\n",
    "#                     for true_score in true_scores:\n",
    "\n",
    "#                         if pred[0] == true_score[0] and pred[2] == true_score[2]:\n",
    "#                             matches_found += 1\n",
    "\n",
    "#                             pred_score = 0\n",
    "#                             current_score = int(float(pred[3]))\n",
    "\n",
    "#                             if lower_bound < int(float(pred[3])) < upper_bound:\n",
    "#                                 pred_score = 1\n",
    "#                             if upper_bound < int(float(pred[3])):\n",
    "#                                 pred_score = 2\n",
    "\n",
    "#                             if pred_score == int(true_score[3]):\n",
    "#                                 correct_preds += 1\n",
    "#                             else:\n",
    "#                                 incorrect_preds += 1\n",
    "\n",
    "#                             break\n",
    "\n",
    "#                 if correct_preds == incorrect_preds == 0:\n",
    "#                     print(\"No matches found, because doc id's do not yet work\")\n",
    "\n",
    "#                 else:\n",
    "#                     performance = correct_preds/(correct_preds+incorrect_preds) * 100\n",
    "#                     if performance > best_performance:\n",
    "                        \n",
    "#                         print(\"For boundaries: [{}, {}]\".format(lower_bound, upper_bound))\n",
    "#                         print(\"Matches found {} of {} predictions \".format(matches_found, len(predictions)))\n",
    "\n",
    "#                         print(\"Performance: {:.2f}%\".format(performance))\n",
    "#                         print(\"Correct predictions: \", correct_preds)\n",
    "#                         print(\"Incorrect predictions: \", incorrect_preds)\n",
    "\n",
    "#                         best_performance = performance\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "class UserInterface:\n",
    "    '''Class used to handle user interaction'''\n",
    "          \n",
    "    def __init__(self):\n",
    "        '''This is the constructor method of the UI'''\n",
    "        print(\"UserInterface has been initiated.\")       \n",
    "        \n",
    "    def getUserQuery(self):\n",
    "        ''''Retrieves the query from the user and returns it'''\n",
    "        userInput = input(\"Please enter a query: \")\n",
    "        return userInput\n",
    "    \n",
    "    def indexAlreadyCreated(self, directory_location):\n",
    "        print(\"Index is set.\")\n",
    "        print(\"Index files are stored in directory:\" + directory_location)\n",
    "        return\n",
    "    \n",
    "    def shouldCreateIndex(self):\n",
    "        print(\"Do you wish to create a new index?\")\n",
    "        userInput = input(\"Answer [y/n]: \")\n",
    "        if userInput == ('y' or 'Y' or \"yes\"):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def indexIsNotSet(self):\n",
    "        print(\"Index has not been set.\")\n",
    "        return\n",
    "    \n",
    "    def getIndexDirectory(self, default_directory):\n",
    "        userInput = input(\"Please enter a directory name for the index (default = /\"+ default_directory + \"): \")\n",
    "        if userInput == \"\":\n",
    "            return default_directory\n",
    "        return userInput\n",
    "    \n",
    "    def getDataPath(self, default_data_path):\n",
    "        print(\"Please enter the path to the TREC_Washington_Post_collection.v2.jl file.\")\n",
    "        print(\"The default location is: /\"+ default_data_path + \"): \")\n",
    "        userInput = input()\n",
    "        if userInput == \"\":\n",
    "            return default_data_path\n",
    "        return userInput\n",
    "    \n",
    "    def creatingIndex(self, index_directory, data_path):\n",
    "        print(\"Creating Index.\")\n",
    "        print(\"Selected index directory:\\t\" + index_directory)\n",
    "        print(\"Selected file to index:\\t\" + data_path)\n",
    "        \n",
    "    def openingIndex(self, index_directory):\n",
    "        print(\"Opening Index.\")\n",
    "        print(\"Selected index directory to use:\\t\" + index_directory)\n",
    "    \n",
    "    def shouldAddExistingIndex(self):\n",
    "        print(\"Do you wish to add an existing index?\")\n",
    "        userInput = input(\"Answer [y/n]: \")\n",
    "        if userInput == ('y' or 'Y' or \"yes\"):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def stopSearchEngine(self):\n",
    "        print(\"Search Engine is stopped.\")\n",
    "        \n",
    "    def shouldTerminateSearchEngine(self):\n",
    "        print(\"Do you wish to stop the search engine?\")\n",
    "        userInput = input(\"Answer [y/n]: \")\n",
    "        if userInput == ('y' or 'Y' or \"yes\"):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def printResults(self, results):\n",
    "        if len(results) > 0:\n",
    "            print(\"\\nPrinting results:\")\n",
    "            counter = 1\n",
    "            for result in results:\n",
    "                print(\"Result \" + str(counter) + \":\")\n",
    "                print(result)\n",
    "                print(\"\")\n",
    "                counter += 1\n",
    "            return\n",
    "        print(\"No results found\")\n",
    "        return\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class SearchEngine:\n",
    "    '''This class embodies the search engine and acts a a controller class'''\n",
    "    \n",
    "    UI = None\n",
    "    Indexer = None\n",
    "    Ranking = None\n",
    "    \n",
    "    RUNNING = False\n",
    "    STOPPED = False\n",
    "    EVALUATION_MODE = True\n",
    "    USER_MODE = False\n",
    "    \n",
    "    user_query = \"\"\n",
    "    DEFAULT_data_path = 'WP-corpus/data/TREC_Washington_Post_collection.v2.jl'\n",
    "    DEFAULT_index_directory = \"indexdir\"\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        print(\"Search Engine has been initiated.\")\n",
    "    \n",
    "    def stopSearchEngine(self, UI):\n",
    "        self.RUNNING = False\n",
    "        UI.stopSearchEngine()        \n",
    "        \n",
    "    def setIndex(self, Indexer, UI):\n",
    "        if Indexer.indexCreated():\n",
    "            directory_location = Indexer.getIndexLocation()\n",
    "            UI.indexAlreadyCreated(directory_location)\n",
    "            return\n",
    "        else:\n",
    "            UI.indexIsNotSet()\n",
    "            if UI.shouldCreateIndex(): # This is for creating a new index\n",
    "                # Index directory (setDir)\n",
    "                index_directory = UI.getIndexDirectory(self.DEFAULT_index_directory)\n",
    "                Indexer.setIndexDirectory(index_directory)\n",
    "                # Check if index_dir exist and makes one if it doesn't\n",
    "                if not os.path.exists(index_directory):\n",
    "                    os.mkdir(index_directory)\n",
    "                \n",
    "                # Data file path (setPath)    \n",
    "                data_path = UI.getDataPath(self.DEFAULT_data_path)\n",
    "                Indexer.setDataPath(data_path)\n",
    "                \n",
    "                # Creating Index\n",
    "                UI.creatingIndex(index_directory, data_path)\n",
    "                Indexer.index()\n",
    "                return\n",
    "                \n",
    "            elif UI.shouldAddExistingIndex(): # This is for adding an existing index\n",
    "                \n",
    "                # TODO add checks to check for existence of index?\n",
    "                \n",
    "                # Index directory, duplicate code -> (setDir)\n",
    "                index_directory = UI.getIndexDirectory(self.DEFAULT_index_directory)\n",
    "                Indexer.setIndexDirectory(index_directory)\n",
    "                # Check if index_dir exist and makes one if it doesn't\n",
    "                if not os.path.exists(index_directory):\n",
    "                    os.mkdir(index_directory)\n",
    "                \n",
    "                # Data file path, duplicate code -> (setPath)    \n",
    "                data_path = UI.getDataPath(self.DEFAULT_data_path)\n",
    "                Indexer.setDataPath(data_path)\n",
    "                \n",
    "                # Set index created to true\n",
    "                UI.openingIndex(index_directory)\n",
    "                Indexer.setIndexCreatedTrue()\n",
    "                return\n",
    "                                \n",
    "            else: # TODO: Ask if the user wants to continue or stop\n",
    "                if UI.shouldTerminateSearchEngine(): \n",
    "                    self.STOPPED = True\n",
    "                return\n",
    "            return\n",
    "        \n",
    "    def evaluateTermFrequency(self, evaluation):\n",
    "        print(\"\\nEvaluating Term Frequency\")\n",
    "        queries  = evaluation.load_queries()\n",
    "        print(\"Queries loaded\")\n",
    "        results_formatted = []\n",
    "        print(\"Performing searches on index\")\n",
    "\n",
    "        for (query, query_id) in tqdm(queries):\n",
    "            results = self.Ranking.searchTermFrequency(query, self.Indexer)\n",
    "            results_formatted = evaluation.addResults(results, results_formatted, query, query_id)\n",
    "\n",
    "        print(\"Writing search results to results.txt\")\n",
    "        evaluation.writeResults(results_formatted)\n",
    "        print(\"Results for Term Frequency: \")\n",
    "        evaluation.evaluateResults()\n",
    "        print('')\n",
    "    \n",
    "    def evaluateTF_IDF(self, evaluation):\n",
    "        print(\"\\nEvaluating TF IDF\")\n",
    "        queries  = evaluation.load_queries()\n",
    "        print(\"Queries loaded\")\n",
    "        results_formatted = []\n",
    "        print(\"Performing searches on index\")\n",
    "\n",
    "        for (query, query_id) in tqdm(queries):\n",
    "            results = self.Ranking.searchTF_IDF(query, self.Indexer)\n",
    "            #results = self.Ranking.searchBM25F(query, self.Indexer)\n",
    "            results_formatted = evaluation.addResults(results, results_formatted, query, query_id)\n",
    "\n",
    "        print(\"Writing search results to results.txt\")\n",
    "        print(\"Results for TF IDF:\")\n",
    "        evaluation.writeResults(results_formatted)\n",
    "        evaluation.evaluateResults()\n",
    "        print('')\n",
    "        \n",
    "    def evaluateBM25F(self, evaluation):\n",
    "        print(\"\\nEvaluating BM25F\")\n",
    "        queries  = evaluation.load_queries()\n",
    "        print(\"Queries loaded\")\n",
    "        results_formatted = []\n",
    "        print(\"Performing searches on index\")\n",
    "\n",
    "        for (query, query_id) in tqdm(queries):\n",
    "            results = self.Ranking.searchBM25F(query, self.Indexer)\n",
    "            results_formatted = evaluation.addResults(results, results_formatted, query, query_id)\n",
    "\n",
    "        print(\"Writing search results to results.txt\")\n",
    "        evaluation.writeResults(results_formatted)\n",
    "        print(\"Results for BM25F:\")\n",
    "        evaluation.evaluateResults()\n",
    "        print('')\n",
    "       \n",
    "    def run(self):\n",
    "        '''This function start the search engine'''\n",
    "        print('Search Engine started.')\n",
    "        \n",
    "        self.UI = UserInterface()\n",
    "        self.Indexer = Indexer()\n",
    "        self.Ranking = Ranking()\n",
    "                \n",
    "        self.RUNNING = True\n",
    "        while self.RUNNING:\n",
    "            if not self.Indexer.indexCreated():\n",
    "                #TODO Inform user that an index needs to be set\n",
    "                self.setIndex(self.Indexer, self.UI)\n",
    "                if self.STOPPED == True: # Stop SE if user did not want to continue\n",
    "                    self.stopSearchEngine(self.UI)\n",
    "                    break\n",
    "            #TODO ask user to choose a mode\n",
    "                       \n",
    "            if self.EVALUATION_MODE:\n",
    "            # This is used to evaluate SE against the TREC relevance judgements\n",
    "                index_dir = self.Indexer.getIndexLocation()\n",
    "                self.Ranking.setIndexDirectory(index_dir)\n",
    "                \n",
    "                evaluation = Evaluation()\n",
    "                \n",
    "                self.evaluateTermFrequency(evaluation)\n",
    "                self.evaluateTF_IDF(evaluation)\n",
    "                self.evaluateBM25F(evaluation)\n",
    "                \n",
    "                print(\"Done\")\n",
    "                self.RUNNING = False\n",
    " \n",
    "            if self.USER_MODE:\n",
    "                # This is used to query questions\n",
    "                #TODO ask for search algorithm\n",
    "                print(\"Entering USER_MODE\")\n",
    "                user_query = self.UI.getUserQuery()\n",
    "                print(\"Entered query: \" + user_query)\n",
    "                \n",
    "                results = self.Ranking.searchTermFrequency(user_query, self.Indexer)\n",
    "                self.UI.printResults(results)\n",
    "                print(\"Starting over.\")\n",
    "                \n",
    "           \n",
    "            print(\"------------------------------------------------------------------------\")\n",
    "            time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Engine has been initiated.\n",
      "Search Engine started.\n",
      "UserInterface has been initiated.\n",
      "Indexer has been initiated.\n",
      "Ranking class has been initiated.\n",
      "Index has not been set.\n",
      "Do you wish to create a new index?\n",
      "Answer [y/n]: n\n",
      "Do you wish to add an existing index?\n",
      "Answer [y/n]: y\n",
      "Please enter a directory name for the index (default = /indexdir): \n",
      "Please enter the path to the TREC_Washington_Post_collection.v2.jl file.\n",
      "The default location is: /WP-corpus/data/TREC_Washington_Post_collection.v2.jl): \n",
      "\n",
      "Opening Index.\n",
      "Selected index directory to use:\tindexdir\n",
      "\n",
      "Evaluating Term Frequency\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queries loaded\n",
      "Performing searches on index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:14<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing search results to results.txt\n",
      "Results for Term Frequency: \n",
      "Average Recall 0.2803283498522676\n",
      "Average Precision 0.19691598885904463\n",
      "Average F_1-Measure 0.1806866382333958\n",
      "\n",
      "\n",
      "Evaluating TF IDF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queries loaded\n",
      "Performing searches on index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:14<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing search results to results.txt\n",
      "Results for TF IDF:\n",
      "Average Recall 0.2803283498522676\n",
      "Average Precision 0.19691598885904463\n",
      "Average F_1-Measure 0.1806866382333958\n",
      "\n",
      "\n",
      "Evaluating BM25F\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queries loaded\n",
      "Performing searches on index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:14<00:00,  3.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing search results to results.txt\n",
      "Results for BM25F:\n",
      "Average Recall 0.2803854927094105\n",
      "Average Precision 0.19706983501289077\n",
      "Average F_1-Measure 0.18076997156672914\n",
      "\n",
      "Done\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mySearchEngine = SearchEngine()\n",
    "mySearchEngine.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
