{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# RUN: 'pip install jsonlines' on: ModuleNotFoundError: No module named 'jsonlines'\n",
    "import json\n",
    "import jsonlines\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# RUN: 'pip install whoosh' on: ModuleNotFoundError: No module named 'whoosh'\n",
    "import os, os.path\n",
    "\n",
    "#from whoosh import index\n",
    "import whoosh\n",
    "from whoosh.index import create_in\n",
    "from whoosh.fields import *\n",
    "from whoosh.writing import AsyncWriter\n",
    "\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "######################################\n",
    "\n",
    "class Indexer:\n",
    "    '''This class is used to create index of the WP database'''\n",
    "    \n",
    "    index_created = False\n",
    "    index_directory = \"\"\n",
    "    data_path = \"\"\n",
    "    # [id, article_url, title, author, publised_date, contents, type, source]\n",
    "    # Define fields using whoosh's 'Schema' | https://whoosh.readthedocs.io/en/latest/schema.html#\n",
    "    # Can add field boost here\n",
    "    schema = Schema(doc_id = whoosh.fields.ID(unique=True, stored=True),\\\n",
    "                       article_url = whoosh.fields.STORED,\\\n",
    "                       title = whoosh.fields.TEXT(stored=True),\\\n",
    "                       author = whoosh.fields.ID,\\\n",
    "                       published_date = whoosh.fields.DATETIME,\\\n",
    "                       contents = whoosh.fields.TEXT)\n",
    "    \n",
    "    def __init__(self):\n",
    "        print('Indexer has been initiated.')\n",
    "        \n",
    "    def indexCreated(self):\n",
    "        return self.index_created\n",
    "      \n",
    "    ## Count documents\n",
    "    def countDocuments(self):\n",
    "        print(\"Counting documents, this might take a while...\")\n",
    "        counter = 0\n",
    "        tenKCounter = 0\n",
    "\n",
    "        with jsonlines.open(self.data_path) as reader:\n",
    "            for obj in tqdm(reader.iter(type=dict, skip_invalid=True)):\n",
    "                tenKCounter += 1\n",
    "                counter += 1\n",
    "            \n",
    "                if tenKCounter >= 10000:\n",
    "                    print(\"Current count is: \" + str(counter))\n",
    "                    tenKCounter = 0 \n",
    "        print(\"Last count is: \" + str(counter))\n",
    "        print(\"Counting done.\")\n",
    "        \n",
    "    def extractDocumentContents(self, contents):\n",
    "        '''Extracts document contents from array of dicts to string'''\n",
    "        build_string = \"\"\n",
    "        json_array = json.dumps(str(contents))\n",
    "        \n",
    "        for item in json_array:\n",
    "            try:\n",
    "                print(str(item['type']))\n",
    "                if str(item['type']) == \"sanitized_html\":\n",
    "                    build_string = build_string + str(item['content'])\n",
    "                    build_string = build_string + \"\\n\"\n",
    "            except:\n",
    "                build_string = \"Could not retreive content\"\n",
    "        \n",
    "        print(build_string)\n",
    "        return build_string\n",
    "    \n",
    "    def extractDocumentDate(self, epochTimestamp):\n",
    "        '''Converst UNIX epoch timestamp to DATETIME'''\n",
    "        if (epochTimestamp == ''):\n",
    "            return datetime(1,1,1)\n",
    "        try:\n",
    "            # Test if source is within specified dates\n",
    "            date_info = epochTimestamp\n",
    "            removed_zeros = str(date_info)[0:10]\n",
    "            timestamp = int(removed_zeros)\n",
    "            return datetime.fromtimestamp(timestamp, timezone('EST'))\n",
    "        except:\n",
    "            print(\"ERROR: In extractDocumentDate()\")\n",
    "            print(\"ERROR: \", sys.exc_info()[0])\n",
    "            #print(\"Caused by value: \" + str(date_info))\n",
    "            return datetime(1,1,1)\n",
    "    \n",
    "    def setIndexDirectory(self, directory):\n",
    "        self.index_directory = directory\n",
    "    \n",
    "    def getIndexLocation(self):\n",
    "        return self.index_directory\n",
    "    \n",
    "    def setDataPath(self, file_location):\n",
    "        self.data_path = file_location\n",
    "    \n",
    "    def getDataPath(self):\n",
    "        return self.data_path\n",
    "    \n",
    "    def setIndexCreatedTrue(self):\n",
    "        self.index_created = True\n",
    "    \n",
    "    def getSchema(self):\n",
    "        return self.schema\n",
    "    \n",
    "    def index(self):\n",
    "        '''This method indexes the data'''\n",
    "        # https://whoosh.readthedocs.io/en/latest/api/writing.html\n",
    "        # https://appliedmachinelearning.blog/2018/07/31/developing-a-fast-indexing-and-full-text-search-engine-with-whoosh-a-pure-python-library/\n",
    "        \n",
    "        assert self.index_directory and self.data_path # Both variables have to be set\n",
    "        \n",
    "        schema = self.getSchema()\n",
    "              \n",
    "        # Creating a index writer to add document as per schema\n",
    "        myindex = whoosh.index.create_in(self.index_directory, schema)\n",
    "        writer = whoosh.writing.AsyncWriter(myindex)\n",
    "        \n",
    "        # Loop over data\n",
    "        print(\"Looping over data. Indexing each article.\")\n",
    "        print(\"This might take a few minutes...\")\n",
    "        counter = 0\n",
    "        checker = 10000\n",
    "        fault_counter = 0\n",
    "        with jsonlines.open(self.data_path) as reader:\n",
    "            for obj in tqdm(reader.iter(type=dict, skip_invalid=True)):\n",
    "                retreived_date = self.extractDocumentDate(obj['published_date'])\n",
    "                if retreived_date != datetime(1,1,1):\n",
    "                    counter = counter + 1\n",
    "                    writer.add_document(doc_id=obj['id'],\\\n",
    "                                        article_url=obj['article_url'],\\\n",
    "                                        title=obj['title'],\\\n",
    "                                        author=obj['author'],\\\n",
    "                                        published_date=retreived_date,\\\n",
    "                                        contents = self.extractDocumentContents(obj['contents']))\n",
    "                else:\n",
    "                    fault_counter = fault_counter + 1\n",
    "                    \n",
    "                if counter > checker:\n",
    "                    writer.commit()\n",
    "                    writer = whoosh.writing.AsyncWriter(myindex)\n",
    "                    checker = checker + 10000\n",
    "                    clear_output(wait=True)\n",
    "                    print(\"Looping over data. Indexing each article.\")\n",
    "                    print(\"This might take a few minutes...\")\n",
    "                    print(\"Indexed \" + str(counter - 1) + \" articles\")\n",
    "                    break\n",
    "                    if fault_counter > 0:\n",
    "                        print(\"Found \" + str(fault_counter) + \" wrongly formatted articles\")\n",
    "            \n",
    "            print(\"Looping complete.\")\n",
    "        print(\"Index created!\")\n",
    "        self.setIndexCreatedTrue()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "from whoosh.qparser import QueryParser, MultifieldParser\n",
    "from whoosh import scoring\n",
    "from whoosh.index import open_dir\n",
    "\n",
    "class Ranking:\n",
    "    '''This class contains functions that are used to create a ranking based on different algorithms'''\n",
    "    \n",
    "    show_n_results = 3\n",
    "    index_directory = \"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        print(\"Ranking class has been initiated.\")\n",
    "    \n",
    "    def indexCreated(self):\n",
    "        return indexCreated\n",
    "    \n",
    "    def setIndexDirectory(self, directory):\n",
    "        self.index_directory = directory\n",
    "    \n",
    "    def openIndex(self):\n",
    "        assert self.index_directory\n",
    "        return open_dir(self.index_directory)\n",
    "    \n",
    "   \n",
    "    def resultsToList(self, results):\n",
    "        results_list = []\n",
    "        for result in results:\n",
    "            result_dict = result.fields()\n",
    "            result_dict['score'] = result.score\n",
    "            results_list.append(result_dict)\n",
    "        return results_list\n",
    "            \n",
    "    \n",
    "    def searchWithSelectedAlgorithm(self, user_query, indexer, scoring_algorithm):\n",
    "    #TODO check if index has been created\n",
    "        index_dir = indexer.getIndexLocation()\n",
    "        self.setIndexDirectory(index_dir)\n",
    "        index = self.openIndex()\n",
    "        results_list = []\n",
    "        schema = indexer.getSchema()\n",
    "        fields = schema.scorable_names()\n",
    "        \n",
    "        with index.searcher(weighting=scoring_algorithm) as searcher:\n",
    "            #parsed_query = QueryParser(\"title\", index.schema).parse(user_query)\n",
    "            parsed_query = MultifieldParser(fields, schema).parse(user_query)\n",
    "            results = searcher.search(parsed_query, limit=self.show_n_results)\n",
    "            results_list = self.resultsToList(results)\n",
    "        \n",
    "        return results_list\n",
    "    \n",
    "    \n",
    "    \n",
    "    def searchTermFrequency(self, user_query, indexer):\n",
    "        '''Returns results for a given query based on the TF-IDF search algorithm. Returned value is a list of dictionaries.'''\n",
    "        scoring_algorithm = scoring.Frequency\n",
    "        return self.searchWithSelectedAlgorithm(user_query, indexer, scoring_algorithm)\n",
    "            \n",
    "    \n",
    "    def searchTF_IDF(self, user_query, indexer):\n",
    "        '''Returns results for a given query based on the TF-IDF search algorithm. Returned value is a list of dictionaries.'''\n",
    "        scoring_algorithm = scoring.TF_IDF\n",
    "        return self.searchWithSelectedAlgorithm(user_query, indexer, scoring_algorithm)\n",
    "    \n",
    "    def searchBM25F(self, user_query, indexer):\n",
    "        '''Returns results for a given query based on the BM25F search algorithm. Returned value is a list of dictionaries.'''\n",
    "        scoring_algorithm = scoring.BM25F(B=0.75, content_B=1.0, K1=1.5)\n",
    "        return self.searchWithSelectedAlgorithm(user_query, indexer, scoring_algorithm)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     def searchTermFrequencyReturnResults(self, user_query):\n",
    "#     #TODO check if index has been created\n",
    "#         index = self.openIndex()\n",
    "\n",
    "#         with index.searcher(weighting=scoring.Frequency) as searcher:\n",
    "#             parsed_query = QueryParser(\"title\", index.schema).parse(user_query)\n",
    "#             results = searcher.search(parsed_query)            \n",
    "#             return results\n",
    "\n",
    "    \n",
    "#     def searchBM25FReturnResults(self, user_query):\n",
    "#         index = self.openIndex()\n",
    "#         with index.searcher(weighting=scoring.BM25F(B=0.75, content_B=1.0, K1=1.5)) as searcher:\n",
    "#             parsed_query = QueryParser(\"title\", index.schema).parse(user_query)\n",
    "#             results = searcher.search(parsed_query)            \n",
    "#             return results  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking class has been initiated.\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "import urllib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Evaluation:\n",
    "    '''Used to evaluate the ranking results'''\n",
    "    #should check if query used is in the \n",
    "    \n",
    "    # TODO load evaluation document and queries\n",
    "    # TODO pass queries on to ranking and obtain ranking results\n",
    "    # TODO compare ranking results to TREC evaluation\n",
    "    # TODO display results, how?\n",
    "    # TODO test different algorithms\n",
    "    \n",
    "    queries = []\n",
    "    ranking = Ranking()\n",
    "    \n",
    "    def load_queries(self):\n",
    "        \n",
    "        query_url = \"https://trec.nist.gov/data/core/topics2018.txt\"\n",
    "        query_file = urllib.request.urlopen(query_url)\n",
    "        titles, numbers = [], []\n",
    "        title = False\n",
    "        \n",
    "        for line in query_file:\n",
    "            decoded_line = line.decode(\"utf-8\")\n",
    "            \n",
    "            if title and not \"</title>\" in decoded_line:\n",
    "                titles.append(decoded_line.replace(\"\\n\", \"\").strip())\n",
    "\n",
    "            if \"<title>\" in decoded_line:\n",
    "                title = True\n",
    "            \n",
    "            if \"</title>\" in decoded_line:\n",
    "                title = False\n",
    "                \n",
    "            if \"<num>\" in decoded_line:\n",
    "                num = decoded_line.replace(\"<num>\", \"\").replace(\"</num>\", \"\").replace(\"\\n\", \"\").replace(\"Number: \", \"\").strip()\n",
    "                numbers.append(num)\n",
    "        \n",
    "        queries = list(zip(titles, numbers))\n",
    "        return queries \n",
    "\n",
    "        \n",
    "    def addResults(self, results_formatted, results, query, number):\n",
    "        count = 0\n",
    "        \n",
    "        for hit in results: \n",
    "            score = int(hit['score'])\n",
    "                \n",
    "            current_result = str(number) + \" 0 \"  + str(hit[\"doc_id\"]) + \" \" + str(score)           \n",
    "            results_formatted.append(current_result)\n",
    "            \n",
    "            count += 1\n",
    "        print(\"for query {} there are {} hits\".format(query, count))\n",
    "        return results_formatted\n",
    "        \n",
    "        \n",
    "    def writeResults(self, results):\n",
    "        \n",
    "        with open(\"results.txt\", \"w\") as results_file:\n",
    "            for result in results:\n",
    "                results_file.write(result + \"\\n\")\n",
    "    \n",
    "    def plotResults(self):\n",
    "        \n",
    "        all_scores = []\n",
    "        \n",
    "        with open(\"results.txt\", \"r\") as results_file:\n",
    "            for line in results_file:\n",
    "                current_result = line.split(\" \")\n",
    "                current_result = [i.strip(\"\\n\") for i in current_result]                    \n",
    "                all_scores.append(int(float(current_result[-1])))\n",
    "                \n",
    "        _ = plt.hist(all_scores, bins='auto')  \n",
    "        plt.title(\"Histogram with 'auto' bins\")\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        print(\"Max score \", max(all_scores))\n",
    "        print(\"Min score \", min(all_scores))\n",
    "    \n",
    "    def compareResults(self):        \n",
    "        # read in results.txt\n",
    "        predictions = []\n",
    "        with open(\"results.txt\", \"r\") as results_file:\n",
    "            for line in results_file:\n",
    "                current_result = line.split(\" \")\n",
    "                current_result = [i.strip(\"\\n\") for i in current_result]\n",
    "                \n",
    "                predictions.append(current_result)\n",
    "                \n",
    "        # read in true scores file\n",
    "        true_scores = []\n",
    "        true_scores_url = \"https://trec.nist.gov/data/core/qrels2018.txt\"\n",
    "        true_scores_file = urllib.request.urlopen(true_scores_url)\n",
    "\n",
    "        for line in true_scores_file:\n",
    "            decoded_line = line.decode(\"utf-8\")\n",
    "            decoded_line = decoded_line.split(\" \")\n",
    "            decoded_line = [i.strip(\"\\n\") for i in decoded_line]\n",
    "            \n",
    "            true_scores.append(decoded_line)\n",
    "        \n",
    "        \n",
    "        # compare results.txt (preds) with true scores\n",
    "        #correct_preds, incorrect_preds = 0, 0\n",
    "        \n",
    "        best_performance = 0\n",
    "        \n",
    "        \n",
    "        for lower_bound in [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24]:\n",
    "            for upper_bound in [26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48]:\n",
    "                \n",
    "                correct_preds, incorrect_preds = 0, 0\n",
    "                matches_found = 0\n",
    "                for pred in predictions:\n",
    "                    for true_score in true_scores:\n",
    "\n",
    "                        if pred[0] == true_score[0] and pred[2] == true_score[2]:\n",
    "                            matches_found += 1\n",
    "\n",
    "                            pred_score = 0\n",
    "                            current_score = int(float(pred[3]))\n",
    "\n",
    "                            if lower_bound < int(float(pred[3])) < upper_bound:\n",
    "                                pred_score = 1\n",
    "                            if upper_bound < int(float(pred[3])):\n",
    "                                pred_score = 2\n",
    "\n",
    "                            if pred_score == int(true_score[3]):\n",
    "                                correct_preds += 1\n",
    "                            else:\n",
    "                                incorrect_preds += 1\n",
    "\n",
    "                            break\n",
    "\n",
    "                if correct_preds == incorrect_preds == 0:\n",
    "                    print(\"No matches found, because doc id's do not yet work\")\n",
    "\n",
    "                else:\n",
    "                    performance = correct_preds/(correct_preds+incorrect_preds) * 100\n",
    "                    if performance > best_performance:\n",
    "                        \n",
    "                        print(\"For boundaries: [{}, {}]\".format(lower_bound, upper_bound))\n",
    "                        print(\"Matches found {} of {} predictions \".format(matches_found, len(predictions)))\n",
    "\n",
    "                        print(\"Performance: {:.2f}%\".format(performance))\n",
    "                        print(\"Correct predictions: \", correct_preds)\n",
    "                        print(\"Incorrect predictions: \", incorrect_preds)\n",
    "\n",
    "                        best_performance = performance\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "class UserInterface:\n",
    "    '''Class used to handle user interaction'''\n",
    "          \n",
    "    def __init__(self):\n",
    "        '''This is the constructor method of the UI'''\n",
    "        print(\"UserInterface has been initiated.\")       \n",
    "        \n",
    "    def getUserQuery(self):\n",
    "        ''''Retrieves the query from the user and returns it'''\n",
    "        userInput = input(\"Please enter a query: \")\n",
    "        return userInput\n",
    "    \n",
    "    def indexAlreadyCreated(self, directory_location):\n",
    "        print(\"Index is set.\")\n",
    "        print(\"Index files are stored in directory:\" + directory_location)\n",
    "        return\n",
    "    \n",
    "    def shouldCreateIndex(self):\n",
    "        print(\"Do you wish to create a new index?\")\n",
    "        userInput = input(\"Answer [y/n]: \")\n",
    "        if userInput == ('y' or 'Y' or \"yes\"):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def indexIsNotSet(self):\n",
    "        print(\"Index has not been set.\")\n",
    "        return\n",
    "    \n",
    "    def getIndexDirectory(self, default_directory):\n",
    "        userInput = input(\"Please enter a directory name for the index (default = /\"+ default_directory + \"): \")\n",
    "        if userInput == \"\":\n",
    "            return default_directory\n",
    "        return userInput\n",
    "    \n",
    "    def getDataPath(self, default_data_path):\n",
    "        print(\"Please enter the path to the TREC_Washington_Post_collection.v2.jl file.\")\n",
    "        print(\"The default location is: /\"+ default_data_path + \"): \")\n",
    "        userInput = input()\n",
    "        if userInput == \"\":\n",
    "            return default_data_path\n",
    "        return userInput\n",
    "    \n",
    "    def creatingIndex(self, index_directory, data_path):\n",
    "        print(\"Creating Index.\")\n",
    "        print(\"Selected index directory:\\t\" + index_directory)\n",
    "        print(\"Selected file to index:\\t\" + data_path)\n",
    "        \n",
    "    def openingIndex(self, index_directory):\n",
    "        print(\"Opening Index.\")\n",
    "        print(\"Selected index directory to use:\\t\" + index_directory)\n",
    "    \n",
    "    def shouldAddExistingIndex(self):\n",
    "        print(\"Do you wish to add an existing index?\")\n",
    "        userInput = input(\"Answer [y/n]: \")\n",
    "        if userInput == ('y' or 'Y' or \"yes\"):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def stopSearchEngine(self):\n",
    "        print(\"Search Engine is stopped.\")\n",
    "        \n",
    "    def shouldTerminateSearchEngine(self):\n",
    "        print(\"Do you wish to stop the search engine?\")\n",
    "        userInput = input(\"Answer [y/n]: \")\n",
    "        if userInput == ('y' or 'Y' or \"yes\"):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def printResults(self, results):\n",
    "        if len(results) > 0:\n",
    "            print(\"\\nPrinting results:\")\n",
    "            counter = 1\n",
    "            for result in results:\n",
    "                print(\"Result \" + str(counter) + \":\")\n",
    "                print(result)\n",
    "                print(\"\")\n",
    "                counter += 1\n",
    "            return\n",
    "        print(\"No results found\")\n",
    "        return\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class SearchEngine:\n",
    "    '''This class embodies the search engine and acts a a controller class'''\n",
    "    \n",
    "    UI = None\n",
    "    Indexer = None\n",
    "    Ranking = None\n",
    "    \n",
    "    RUNNING = False\n",
    "    STOPPED = False\n",
    "    EVALUATION_MODE = True\n",
    "    USER_MODE = False\n",
    "    \n",
    "    user_query = \"\"\n",
    "    DEFAULT_data_path = 'WP-corpus/data/TREC_Washington_Post_collection.v2.jl'\n",
    "    DEFAULT_index_directory = \"indexdir\"\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        print(\"Search Engine has been initiated.\")\n",
    "    \n",
    "    def stopSearchEngine(self, UI):\n",
    "        self.RUNNING = False\n",
    "        UI.stopSearchEngine()        \n",
    "        \n",
    "    def setIndex(self, Indexer, UI):\n",
    "        if Indexer.indexCreated():\n",
    "            directory_location = Indexer.getIndexLocation()\n",
    "            UI.indexAlreadyCreated(directory_location)\n",
    "            return\n",
    "        else:\n",
    "            UI.indexIsNotSet()\n",
    "            if UI.shouldCreateIndex(): # This is for creating a new index\n",
    "                # Index directory (setDir)\n",
    "                index_directory = UI.getIndexDirectory(self.DEFAULT_index_directory)\n",
    "                Indexer.setIndexDirectory(index_directory)\n",
    "                # Check if index_dir exist and makes one if it doesn't\n",
    "                if not os.path.exists(index_directory):\n",
    "                    os.mkdir(index_directory)\n",
    "                \n",
    "                # Data file path (setPath)    \n",
    "                data_path = UI.getDataPath(self.DEFAULT_data_path)\n",
    "                Indexer.setDataPath(data_path)\n",
    "                \n",
    "                # Creating Index\n",
    "                UI.creatingIndex(index_directory, data_path)\n",
    "                Indexer.index()\n",
    "                return\n",
    "                \n",
    "            elif UI.shouldAddExistingIndex(): # This is for adding an existing index\n",
    "                \n",
    "                # TODO add checks to check for existence of index?\n",
    "                \n",
    "                # Index directory, duplicate code -> (setDir)\n",
    "                index_directory = UI.getIndexDirectory(self.DEFAULT_index_directory)\n",
    "                Indexer.setIndexDirectory(index_directory)\n",
    "                # Check if index_dir exist and makes one if it doesn't\n",
    "                if not os.path.exists(index_directory):\n",
    "                    os.mkdir(index_directory)\n",
    "                \n",
    "                # Data file path, duplicate code -> (setPath)    \n",
    "                data_path = UI.getDataPath(self.DEFAULT_data_path)\n",
    "                Indexer.setDataPath(data_path)\n",
    "                \n",
    "                # Set index created to true\n",
    "                UI.openingIndex(index_directory)\n",
    "                Indexer.setIndexCreatedTrue()\n",
    "                return\n",
    "                                \n",
    "            else: # TODO: Ask if the user wants to continue or stop\n",
    "                if UI.shouldTerminateSearchEngine(): \n",
    "                    self.STOPPED = True\n",
    "                return\n",
    "            return\n",
    "       \n",
    "    def run(self):\n",
    "        '''This function start the search engine'''\n",
    "        print('Search Engine started.')\n",
    "        \n",
    "        self.UI = UserInterface()\n",
    "        self.Indexer = Indexer()\n",
    "        self.Ranking = Ranking()\n",
    "                \n",
    "        self.RUNNING = True\n",
    "        while self.RUNNING:\n",
    "            if not self.Indexer.indexCreated():\n",
    "                #TODO Inform user that an index needs to be set\n",
    "                self.setIndex(self.Indexer, self.UI)\n",
    "                if self.STOPPED == True: # Stop SE if user did not want to continue\n",
    "                    self.stopSearchEngine(self.UI)\n",
    "                    break\n",
    "            #TODO ask user to choose a mode\n",
    "                       \n",
    "            if self.EVALUATION_MODE:\n",
    "            # This is used to evaluate SE against the TREC relevance judgements\n",
    "                index_dir = self.Indexer.getIndexLocation()\n",
    "                self.Ranking.setIndexDirectory(index_dir)\n",
    "                \n",
    "                evaluation = Evaluation()\n",
    "                queries  = evaluation.load_queries()\n",
    "                print(\"Queries loaded\")\n",
    "                \n",
    "                results_formatted = []\n",
    "                \n",
    "                for (query, number) in tqdm(queries):\n",
    "                    #results = self.Ranking.searchTermFrequencyReturnResults(query)\n",
    "                    results = self.Ranking.searchBM25F(query, self.Indexer)\n",
    "                    results_formatted = evaluation.addResults(results_formatted, results, query, number)\n",
    "                \n",
    "                evaluation.writeResults(results_formatted)\n",
    "                evaluation.plotResults()\n",
    "                evaluation.compareResults()  \n",
    "                \n",
    "                print(\"Done\")\n",
    "                self.RUNNING = False\n",
    " \n",
    "            if self.USER_MODE:\n",
    "                # This is used to query questions\n",
    "                #TODO ask for search algorithm\n",
    "                #TODO ask for user query\n",
    "                #TODO move printing results to UI\n",
    "                print(\"Entering USER_MODE\")\n",
    "                user_query = self.UI.getUserQuery()\n",
    "                print(\"Entered query: \" + user_query)\n",
    "                \n",
    "                results = self.Ranking.searchTermFrequency(user_query, self.Indexer)\n",
    "                self.UI.printResults(results)\n",
    "                \n",
    "            print(\"Starting over.\")\n",
    "            print(\"------------------------------------------------------------------------\")\n",
    "            time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Engine has been initiated.\n",
      "Search Engine started.\n",
      "UserInterface has been initiated.\n",
      "Indexer has been initiated.\n",
      "Ranking class has been initiated.\n",
      "Index has not been set.\n",
      "Do you wish to create a new index?\n",
      "Answer [y/n]: n\n",
      "Do you wish to add an existing index?\n",
      "Answer [y/n]: y\n",
      "Please enter a directory name for the index (default = /indexdir): \n",
      "Please enter the path to the TREC_Washington_Post_collection.v2.jl file.\n",
      "The default location is: /WP-corpus/data/TREC_Washington_Post_collection.v2.jl): \n",
      "\n",
      "Opening Index.\n",
      "Selected index directory to use:\tindexdir\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▋                                                                                 | 1/50 [00:00<00:05,  8.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queries loaded\n",
      "for query Women in Parliaments there are 3 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|████▉                                                                              | 3/50 [00:00<00:06,  7.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query Black Bear Attacks there are 3 hits\n",
      "for query Airport Security there are 3 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████████▎                                                                          | 5/50 [00:00<00:05,  7.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query Wildlife Extinction there are 3 hits\n",
      "for query Health and Computer Terminals there are 1 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█████████████▎                                                                     | 8/50 [00:01<00:05,  8.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query human smuggling there are 3 hits\n",
      "for query transportation tunnel disasters there are 3 hits\n",
      "for query piracy there are 3 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▍                                                                 | 10/50 [00:01<00:05,  7.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query hydrogen energy there are 3 hits\n",
      "for query euro opposition there are 3 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|███████████████████▋                                                              | 12/50 [00:01<00:04,  8.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query mercy killing there are 3 hits\n",
      "for query automobile recalls there are 3 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██████████████████████▉                                                           | 14/50 [00:01<00:04,  8.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query Amazon rain forest there are 3 hits\n",
      "for query tropical storms there are 3 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|██████████████████████████▏                                                       | 16/50 [00:01<00:04,  8.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query Cuba, sugar, exports there are 0 hits\n",
      "for query art, stolen, forged there are 1 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|█████████████████████████████▌                                                    | 18/50 [00:02<00:03,  8.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query law enforcement, dogs there are 3 hits\n",
      "for query UV damage, eyes there are 3 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████████████████████████████████▊                                                 | 20/50 [00:02<00:03,  8.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query Greek, philosophy, stoicism there are 0 hits\n",
      "for query inventions, scientific discoveries there are 2 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████████████████████████████████████                                              | 22/50 [00:02<00:03,  8.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query heroic acts there are 3 hits\n",
      "for query women clergy there are 3 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|███████████████████████████████████████▎                                          | 24/50 [00:02<00:03,  7.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query human stampede there are 3 hits\n",
      "for query food stamps increase there are 3 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|██████████████████████████████████████████▋                                       | 26/50 [00:03<00:03,  7.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query college education advantage there are 3 hits\n",
      "for query Africa polio vaccination there are 2 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████████████████████████████████████████████▉                                    | 28/50 [00:03<00:03,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query women driving in Saudi Arabia there are 3 hits\n",
      "for query declining middle class in U.S. there are 3 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████████████████████████████▏                                | 30/50 [00:03<00:02,  7.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query \"Women on 20s\" there are 3 hits\n",
      "for query eating invasive species there are 3 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|████████████████████████████████████████████████████▍                             | 32/50 [00:04<00:02,  7.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query computers and paralyzed people there are 3 hits\n",
      "for query Chavez medical treatment in Cuba there are 3 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|███████████████████████████████████████████████████████▊                          | 34/50 [00:04<00:01,  8.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query Boston marathon bombing verdict there are 3 hits\n",
      "for query protect Earth from asteroids there are 3 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████████████████████████████████████████████████████████                       | 36/50 [00:04<00:01,  8.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query diabetes and toxic chemicals there are 3 hits\n",
      "for query car hacking there are 3 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|██████████████████████████████████████████████████████████████▎                   | 38/50 [00:04<00:01,  8.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query social media and teen suicide there are 3 hits\n",
      "for query marijuana potency there are 3 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|█████████████████████████████████████████████████████████████████▌                | 40/50 [00:05<00:01,  7.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query China one-child impact there are 3 hits\n",
      "for query Jason Rezaian released from Iran there are 3 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████████████████████████████████████████████████████████████████▉             | 42/50 [00:05<00:01,  7.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query federal minimum wage increase there are 3 hits\n",
      "for query Alan Gross released by Cuba there are 3 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████████████████████████████████████████████████████████████████████▏         | 44/50 [00:05<00:00,  8.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query eggs in a healthy diet there are 3 hits\n",
      "for query U.S. age demographics there are 3 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|███████████████████████████████████████████████████████████████████████████▍      | 46/50 [00:05<00:00,  8.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query bacterial infection mortality rate there are 1 hits\n",
      "for query email scams there are 3 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|██████████████████████████████████████████████████████████████████████████████▋   | 48/50 [00:05<00:00,  8.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query Sony cyberattack there are 3 hits\n",
      "for query control of MRSA there are 3 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [00:06<00:00,  8.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for query Bezos purchases Washington Post there are 3 hits\n",
      "for query ethanol and food prices there are 3 hits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max score  56\n",
      "Min score  7\n",
      "For boundaries: [0, 26]\n",
      "Matches found 133 of 136 predictions \n",
      "Performance: 22.56%\n",
      "Correct predictions:  30\n",
      "Incorrect predictions:  103\n",
      "For boundaries: [8, 26]\n",
      "Matches found 133 of 136 predictions \n",
      "Performance: 24.81%\n",
      "Correct predictions:  33\n",
      "Incorrect predictions:  100\n",
      "For boundaries: [12, 26]\n",
      "Matches found 133 of 136 predictions \n",
      "Performance: 27.07%\n",
      "Correct predictions:  36\n",
      "Incorrect predictions:  97\n",
      "For boundaries: [14, 26]\n",
      "Matches found 133 of 136 predictions \n",
      "Performance: 27.82%\n",
      "Correct predictions:  37\n",
      "Incorrect predictions:  96\n",
      "For boundaries: [16, 26]\n",
      "Matches found 133 of 136 predictions \n",
      "Performance: 31.58%\n",
      "Correct predictions:  42\n",
      "Incorrect predictions:  91\n",
      "For boundaries: [18, 26]\n",
      "Matches found 133 of 136 predictions \n",
      "Performance: 32.33%\n",
      "Correct predictions:  43\n",
      "Incorrect predictions:  90\n",
      "For boundaries: [20, 26]\n",
      "Matches found 133 of 136 predictions \n",
      "Performance: 37.59%\n",
      "Correct predictions:  50\n",
      "Incorrect predictions:  83\n",
      "For boundaries: [22, 26]\n",
      "Matches found 133 of 136 predictions \n",
      "Performance: 41.35%\n",
      "Correct predictions:  55\n",
      "Incorrect predictions:  78\n",
      "For boundaries: [24, 26]\n",
      "Matches found 133 of 136 predictions \n",
      "Performance: 50.38%\n",
      "Correct predictions:  67\n",
      "Incorrect predictions:  66\n",
      "Done\n",
      "Starting over.\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mySearchEngine = SearchEngine()\n",
    "mySearchEngine.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
